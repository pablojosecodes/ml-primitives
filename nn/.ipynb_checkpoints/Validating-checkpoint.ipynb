{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11eea261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_MNIST():\n",
    "    \n",
    "    # Load MNIST data using Keras\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    # Reshape and normalize image data\n",
    "    train_images = train_images.reshape(train_images.shape[0], 28*28)\n",
    "    train_images = train_images.astype('float32') / 255\n",
    "    test_images = test_images.reshape(test_images.shape[0], 28*28)\n",
    "    test_images = test_images.astype('float32') / 255\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "    # Convert data to a list of tuples as required by the Network class\n",
    "    training_data = list(zip([np.reshape(x, (784, 1)) for x in train_images], [np.reshape(y, (10, 1)) for y in train_labels]))\n",
    "    test_data = list(zip([np.reshape(x, (784, 1)) for x in test_images], [np.reshape(y, (10, 1)) for y in test_labels]))\n",
    "    return (training_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb57f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions: [784, 30, 10]\n",
    "\n",
    "# Set up some training data before recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c326eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.biases = [np.random.randn(l,1) for l in sizes[1:]]\n",
    "        self.weights = [np.random.randn(l,r) for r, l in zip(sizes[:-1], sizes[1:])]\n",
    "        self.n = len(sizes)\n",
    "    \n",
    "    \n",
    "    # Just runs through the weights and returns final layer's activations (result)\n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            a = np.dot(w,a) + b\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def mini_batch_update(self, batch, eta):\n",
    "        \n",
    "        # What we'll update our weights + biases with after computing\n",
    "        # the gradient of cost function\n",
    "        w_update = [np.zeros(w.shape) for w in self.weights]\n",
    "        b_update = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        for x, y in batch:\n",
    "            w,b = self.backprop(x,y)\n",
    "            \n",
    "            # small updates\n",
    "            w_update = [w+nw for w,nw in zip(w_update, w)]\n",
    "            b_update = [b+nb for b,nb in zip(b_update, b)]\n",
    "        \n",
    "        # Now update the actual parameters!\n",
    "        \n",
    "        self.weights = [w - (eta/len(batch)) * update for w, update in zip(self.weights, w_update) ]\n",
    "        self.biases = [b - (eta/len(batch)) * update for b, update in zip(self.biases, b_update) ]\n",
    "        \n",
    "    # now for the most exciting part!!!\n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        # Feedforward: we need all the activations and weighted inputs zs\n",
    "        activation = x\n",
    "        activations = [activation]\n",
    "        zs = []\n",
    "        \n",
    "        w_nabla = [np.zeros(w.shape) for w in self.weights]\n",
    "        b_nabla = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        \n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            \n",
    "            z = np.dot(w,activation) + b\n",
    "            \n",
    "            \n",
    "            activation = sigmoid(z)\n",
    "            zs.append(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # Final Layer\n",
    "        delta = self.cost_derivative(activations[-1],y) * sigmoid_prime(zs[-1])\n",
    "        \n",
    "        # Now we can update the nabla_b and nabla_b for final layer\n",
    "        b_nabla[-1] = delta\n",
    "        w_nabla[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "        # Backprop\n",
    "        for l in range(2,self.n):\n",
    "            delta = np.dot(self.weights[-l+1].transpose(),delta) * sigmoid_prime(zs[-l])\n",
    "            b_nabla[-l] = delta\n",
    "            w_nabla[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (w_nabla, b_nabla)\n",
    "        \n",
    "        \n",
    "    # we're using a very simple cost function\n",
    "    def cost_derivative(self, final_activation, actual_output):\n",
    "        return (final_activation - actual_output)\n",
    "        \n",
    "        \n",
    "    # Training!\n",
    "    def SGD(self, training_data, epochs, batch_size, eta, test_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[x:x+batch_size] for x in range(0,len(training_data), batch_size)]\n",
    "            \n",
    "            for batch in batches:\n",
    "                self.mini_batch_update(batch, eta)\n",
    "                \n",
    "            if (test_data):\n",
    "                print(f\"Epoch {epoch}: {self.evaluate(test_data) / len(test_data)}\")\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \n",
    "        # x: input layer\n",
    "        # y: right answer\n",
    "        results = [(int(np.argmax(self.feedforward(x))), int(np.argmax(y))) for x, y in test_data]\n",
    "        \n",
    "#         print(type(results[0][0]))\n",
    "#         print(type(results[0][1]))\n",
    "        correct = 0\n",
    "        for _ in range(len(results)):\n",
    "            if (results[_][0]==results[_][1]):\n",
    "                correct += 1\n",
    "        return correct\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1.0-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bab6745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "86c22f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a74f1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.262\n",
      "Epoch 1: 0.3282\n",
      "Epoch 2: 0.3697\n",
      "Epoch 3: 0.4004\n",
      "Epoch 4: 0.4189\n",
      "Epoch 5: 0.4372\n",
      "Epoch 6: 0.4536\n",
      "Epoch 7: 0.4739\n",
      "Epoch 8: 0.4998\n",
      "Epoch 9: 0.5218\n",
      "Epoch 10: 0.5397\n",
      "Epoch 11: 0.5562\n",
      "Epoch 12: 0.5762\n",
      "Epoch 13: 0.6097\n",
      "Epoch 14: 0.6342\n",
      "Epoch 15: 0.6443\n",
      "Epoch 16: 0.6526\n",
      "Epoch 17: 0.6566\n",
      "Epoch 18: 0.6631\n",
      "Epoch 19: 0.6668\n",
      "Epoch 20: 0.6716\n",
      "Epoch 21: 0.6747\n",
      "Epoch 22: 0.6778\n",
      "Epoch 23: 0.6814\n",
      "Epoch 24: 0.6853\n",
      "Epoch 25: 0.6888\n",
      "Epoch 26: 0.6896\n",
      "Epoch 27: 0.694\n",
      "Epoch 28: 0.6952\n",
      "Epoch 29: 0.6971\n"
     ]
    }
   ],
   "source": [
    "nn.SGD(training_data,\n",
    "       30, batch_size=1000, eta=3, test_data=test_data)\n",
    "\n",
    "# Not good! Something's wrong. lets try a higher learning rate. more promising\n",
    "# Let's try on full data size\n",
    "# Solidly better than random! Let's see if it keeps improving...\n",
    "# hmmmm let's fix it\n",
    "# Back from break! Let's first make sure that the hyperparameters\n",
    "# aren't causing the issue.\n",
    "# We've experimented with eta, but batch size is still ridiculously large\n",
    "# - 6 batches for all the data\n",
    "# Let's try smaller batch size- should learn muc h faster.\n",
    "# That's better! Let's skip to the end of the 30 epochs.\n",
    "\n",
    "# It worked! Not super high accuracy, but high enough to validate our code works!\n",
    "# Thanks for watching!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4158147",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.evaluate(test_data) / len(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307d6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f549410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ae36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
